{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2d88c-e6a5-4e1d-9b04-75159897e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from PIL import Image, ImageFile\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class CustomSegmentationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, transformations=None):\n",
    "        all_images = sorted(glob(f\"{root}/images/images/*\"))\n",
    "        \n",
    "        pairs = []\n",
    "        for img_path in all_images:\n",
    "            label_path = f\"{root}/label/label/{self.get_filename(img_path)}_label.tif\"\n",
    "            if os.path.exists(label_path):\n",
    "                pairs.append((img_path, label_path))\n",
    "        \n",
    "        self.image_paths = [p[0] for p in pairs]\n",
    "        self.label_paths = [p[1] for p in pairs]\n",
    "        self.transformations = transformations\n",
    "        self.n_cls = 2\n",
    "        \n",
    "        assert len(self.image_paths) == len(self.label_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.load_image_pair(self.image_paths[idx], self.label_paths[idx])\n",
    "        \n",
    "        if self.transformations:\n",
    "            transformed = self.transformations(image=img, mask=label)\n",
    "            img = transformed[\"image\"]\n",
    "            label = transformed[\"mask\"]\n",
    "        \n",
    "        return img, (label / 255).int()\n",
    "        \n",
    "    def get_filename(self, path):\n",
    "        return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    def load_image_pair(self, img_path, label_path):\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        label = np.array(Image.open(label_path).convert(\"L\"))\n",
    "        return img, label\n",
    "\n",
    "def get_dataloaders(root, transformations, batch_size, split=[0.9, 0.05, 0.05], num_workers=0):\n",
    "    assert abs(sum(split) - 1.0) < 0.001, \"Split ratios must sum to 1\"\n",
    "    \n",
    "    dataset = CustomSegmentationDataset(root=root, transformations=transformations)\n",
    "    n_classes = dataset.n_cls\n",
    "    \n",
    "    total = len(dataset)\n",
    "    train_size = int(total * split[0])\n",
    "    val_size = int(total * split[1])\n",
    "    test_size = total - train_size - val_size\n",
    "    \n",
    "    train_ds, val_ds, test_ds = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain set: {len(train_ds)} images\")\n",
    "    print(f\"Validation set: {len(val_ds)} images\")\n",
    "    print(f\"Test set: {len(test_ds)} images\\n\")\n",
    "    \n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_dl = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_dl, val_dl, test_dl, n_classes\n",
    "\n",
    "root = \"dataset\"\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "img_size = 256\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "tr_dl, val_dl, test_dl, n_cls = get_dataloaders(root=root, transformations=transform, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebcd2fb-435f-4748-999a-a67bf87be4c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f8b4d0-2931-482a-a515-6362d0463d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms as tfs\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    denorm1 = tfs.Normalize(mean=[0., 0., 0.], std=[1/0.229, 1/0.224, 1/0.225])\n",
    "    denorm2 = tfs.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.])\n",
    "    inv_transform = tfs.Compose([denorm1, denorm2])\n",
    "    \n",
    "    is_rgb = len(tensor.shape) == 3\n",
    "    \n",
    "    if is_rgb:\n",
    "        img = inv_transform(tensor) * 255\n",
    "        img = img.detach().cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    else:\n",
    "        img = (tensor * 255).detach().cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def plot_image(rows, cols, pos, img, is_mask=False, title=\"Image\"):\n",
    "    plt.subplot(rows, cols, pos)\n",
    "    if is_mask:\n",
    "        plt.imshow(tensor_to_numpy(img.squeeze(0).float()))\n",
    "    else:\n",
    "        plt.imshow(tensor_to_numpy(img.squeeze(0)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    return pos + 1\n",
    "\n",
    "def visualize_dataset(dataset, num_samples=20):\n",
    "    plt.figure(figsize=(25, 20))\n",
    "    rows = num_samples // 4\n",
    "    cols = num_samples // rows\n",
    "    pos = 1\n",
    "    \n",
    "    random_indices = [random.randint(0, len(dataset) - 1) for _ in range(num_samples)]\n",
    "    \n",
    "    for idx in random_indices:\n",
    "        if pos > num_samples:\n",
    "            break\n",
    "        img, mask = dataset[idx]\n",
    "        \n",
    "        pos = plot_image(rows, cols, pos, img)\n",
    "        pos = plot_image(rows, cols, pos, mask, is_mask=True, title=\"Ground Truth\")\n",
    "        \n",
    "visualize_dataset(tr_dl.dataset, num_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36043c99-675f-47d9-b8d7-636159715945",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0dfbd4-aa44-455d-a7fe-f5c3c4f6480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models_pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet50\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=n_cls,\n",
    ")\n",
    "\n",
    "class CombinedLoss(torch.nn.Module):\n",
    "    def __init__(self, ce_weight=0.6, dice_weight=0.4):\n",
    "        super().__init__()\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        \n",
    "    def dice(self, pred, target):\n",
    "        smooth = 1.0\n",
    "        pred_soft = F.softmax(pred, dim=1)\n",
    "        pred_hot = F.one_hot(torch.argmax(pred_soft, dim=1), n_cls).permute(0, 3, 1, 2).float()\n",
    "        target_hot = F.one_hot(target.squeeze(1).long(), n_cls).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        inter = (pred_hot * target_hot).sum(dim=(2, 3))\n",
    "        union = pred_hot.sum(dim=(2, 3)) + target_hot.sum(dim=(2, 3))\n",
    "        dice_score = (2.0 * inter + smooth) / (union + smooth)\n",
    "        return 1 - dice_score.mean()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = self.ce(pred, target.squeeze(1).long())\n",
    "        dice_loss = self.dice(pred, target)\n",
    "        return self.ce_weight * ce_loss + self.dice_weight * dice_loss\n",
    "\n",
    "loss_fn = CombinedLoss(ce_weight=0.6, dice_weight=0.4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, pred, gt, loss_fn, eps=1e-10, n_cls=2):\n",
    "        self.pred = torch.argmax(pred, dim=1)\n",
    "        self.gt = gt.squeeze(1)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.eps = eps\n",
    "        self.n_cls = n_cls\n",
    "        self.pred_full = pred\n",
    "        \n",
    "    def to_flat(self, x):\n",
    "        return x.contiguous().view(-1)\n",
    "    \n",
    "    def pixel_accuracy(self):\n",
    "        with torch.no_grad():\n",
    "            correct = torch.eq(self.pred, self.gt).int()\n",
    "        return float(correct.sum()) / float(correct.numel())\n",
    "\n",
    "    def mean_iou(self):\n",
    "        with torch.no_grad():\n",
    "            pred_flat = self.to_flat(self.pred)\n",
    "            gt_flat = self.to_flat(self.gt)\n",
    "            \n",
    "            ious = []\n",
    "            for cls in range(self.n_cls):\n",
    "                pred_cls = pred_flat == cls\n",
    "                gt_cls = gt_flat == cls\n",
    "                \n",
    "                if gt_cls.long().sum().item() == 0:\n",
    "                    ious.append(np.nan)\n",
    "                else:\n",
    "                    inter = torch.logical_and(pred_cls, gt_cls).sum().float().item()\n",
    "                    union = torch.logical_or(pred_cls, gt_cls).sum().float().item()\n",
    "                    iou = (inter + self.eps) / (union + self.eps)\n",
    "                    ious.append(iou)\n",
    "                    \n",
    "            return np.nanmean(ious)\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        return self.loss_fn(self.pred_full, self.gt.long())\n",
    "\n",
    "def timer(start=None):\n",
    "    if start is None:\n",
    "        return time.time()\n",
    "    return time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9c433d-f7e4-4838-8827-20e1e4e12417",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a807681-2af0-4cb6-bce3-b99c434c9493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_fn, optimizer, scheduler, scaler, device, epochs, save_prefix, threshold=0.005, save_path=\"saved_models\", grad_clip=1.0):\n",
    "    train_losses, train_pa, train_iou = [], [], []\n",
    "    val_losses, val_pa, val_iou = [], [], []\n",
    "    train_batches = len(train_loader)\n",
    "    val_batches = len(val_loader)\n",
    "    best_iou = 0.0\n",
    "    best_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    early_stop = 10\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.to(device)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        print(f\"Model on GPU: {next(model.parameters()).is_cuda}\")\n",
    "    \n",
    "    start_time = timer()\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start = timer()\n",
    "        train_loss, train_iou_val, train_pa_val = 0, 0, 0\n",
    "        \n",
    "        model.train()\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Training\"):\n",
    "            images, masks = batch\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                predictions = model(images)\n",
    "                metrics = Metrics(predictions, masks, loss_fn, n_cls=n_cls)\n",
    "                loss = metrics.compute_loss()\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                train_iou_val += metrics.mean_iou()\n",
    "                train_pa_val += metrics.pixel_accuracy()\n",
    "                train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_iou_val, val_pa_val = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validation\"):\n",
    "                images, masks = batch\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "                \n",
    "                with autocast():\n",
    "                    predictions = model(images)\n",
    "                    metrics = Metrics(predictions, masks, loss_fn, n_cls=n_cls)\n",
    "                \n",
    "                val_loss += metrics.compute_loss().item()\n",
    "                val_iou_val += metrics.mean_iou()\n",
    "                val_pa_val += metrics.pixel_accuracy()\n",
    "        \n",
    "        train_loss /= train_batches\n",
    "        train_iou_val /= train_batches\n",
    "        train_pa_val /= train_batches\n",
    "        \n",
    "        val_loss /= val_batches\n",
    "        val_iou_val /= val_batches\n",
    "        val_pa_val /= val_batches\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch} Results:\")\n",
    "        print(f\"Time: {timer(epoch_start):.2f}s | LR: {current_lr:.6f}\")\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, PA: {train_pa_val:.4f}, IoU: {train_iou_val:.4f}\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, PA: {val_pa_val:.4f}, IoU: {val_iou_val:.4f}\\n\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_iou.append(train_iou_val)\n",
    "        train_pa.append(train_pa_val)\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_iou.append(val_iou_val)\n",
    "        val_pa.append(val_pa_val)\n",
    "        \n",
    "        improved = False\n",
    "        if val_iou_val > best_iou:\n",
    "            print(f\"IoU improved: {best_iou:.4f} -> {val_iou_val:.4f}\")\n",
    "            best_iou = val_iou_val\n",
    "            improved = True\n",
    "            no_improve = 0\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_iou': best_iou,\n",
    "                'val_loss': val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, f\"{save_path}/{save_prefix}_best_model_iou.pt\")\n",
    "            print(f\"Saved model (IoU: {best_iou:.4f})\\n\")\n",
    "        \n",
    "        if val_loss < best_loss - threshold:\n",
    "            print(f\"Loss improved: {best_loss:.4f} -> {val_loss:.4f}\")\n",
    "            best_loss = val_loss\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "                'val_iou': val_iou_val,\n",
    "            }\n",
    "            torch.save(checkpoint, f\"{save_path}/{save_prefix}_best_model_loss.pt\")\n",
    "            print(f\"Saved model (Loss: {best_loss:.4f})\\n\")\n",
    "        \n",
    "        if not improved:\n",
    "            no_improve += 1\n",
    "            print(f\"No improvement for {no_improve} epochs\")\n",
    "            if no_improve >= early_stop:\n",
    "                print(f\"\\nEarly stopping after {no_improve} epochs\\n\")\n",
    "                break\n",
    "    \n",
    "    total_time = timer(start_time) / 60\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} minutes\")\n",
    "    print(f\"Best IoU: {best_iou:.4f} | Best Loss: {best_loss:.4f}\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"tr_loss\": train_losses, \"tr_iou\": train_iou, \"tr_pa\": train_pa,\n",
    "        \"val_loss\": val_losses, \"val_iou\": val_iou, \"val_pa\": val_pa,\n",
    "        \"best_iou\": best_iou, \"best_loss\": best_loss\n",
    "    }\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {device}...\")\n",
    "\n",
    "history = train(\n",
    "    model=model,\n",
    "    train_loader=tr_dl,\n",
    "    val_loader=val_dl,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    epochs=50,\n",
    "    save_prefix=\"rooftop\",\n",
    "    grad_clip=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a9526-47f3-4303-a6fe-5e1360b22d89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafb756-335a-40fb-ba36-ce5a7c4ffc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningCurves:\n",
    "    def __init__(self, results):\n",
    "        self.results = results\n",
    "        self.plot_curve(\"tr_iou\", \"val_iou\", \"Train IoU\", \"Val IoU\", \"IoU\", \"IoU Score\")\n",
    "        self.plot_curve(\"tr_pa\", \"val_pa\", \"Train PA\", \"Val PA\", \"Pixel Accuracy\", \"PA Score\")\n",
    "        self.plot_curve(\"tr_loss\", \"val_loss\", \"Train Loss\", \"Val Loss\", \"Loss\", \"Loss Value\")\n",
    "    \n",
    "    def plot_curve(self, train_key, val_key, train_label, val_label, title, ylabel):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.results[train_key], label=train_label)\n",
    "        plt.plot(self.results[val_key], label=val_label)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "LearningCurves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46c19a-6f2d-4fb7-b9c9-54c0ad26d3ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc2360-5fb2-42ab-8d57-3fb9437e78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(data_loader, model, device, num_images=15):\n",
    "    cols = num_images // 3\n",
    "    rows = num_images // cols\n",
    "    \n",
    "    images_list, masks_list, predictions_list = [], [], []\n",
    "    \n",
    "    for data in data_loader:\n",
    "        img, mask = data\n",
    "        with torch.no_grad():\n",
    "            pred = torch.argmax(model(img.to(device)), dim=1)\n",
    "        images_list.append(img)\n",
    "        masks_list.append(mask)\n",
    "        predictions_list.append(pred)\n",
    "    \n",
    "    plt.figure(figsize=(25, 20))\n",
    "    pos = 1\n",
    "    \n",
    "    for i, (img, mask, pred) in enumerate(zip(images_list, masks_list, predictions_list)):\n",
    "        if i >= cols:\n",
    "            break\n",
    "        \n",
    "        pos = plot_image(cols, rows, pos, img, title=\"Original\")\n",
    "        pos = plot_image(cols, rows, pos, mask.squeeze(0), is_mask=True, title=\"Ground Truth\")\n",
    "        pos = plot_image(cols, rows, pos, pred, is_mask=True, title=\"Prediction\")\n",
    "\n",
    "import os\n",
    "iou_path = \"saved_models/rooftop_best_model_iou.pt\"\n",
    "loss_path = \"saved_models/rooftop_best_model_loss.pt\"\n",
    "old_path = \"saved_models/rooftop_best_model.pt\"\n",
    "\n",
    "if os.path.exists(iou_path):\n",
    "    print(f\"Loading model from {iou_path}\")\n",
    "    checkpoint = torch.load(iou_path, map_location=device)\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Best IoU: {checkpoint.get('best_iou', 'N/A')}\")\n",
    "    else:\n",
    "        model = checkpoint\n",
    "elif os.path.exists(loss_path):\n",
    "    print(f\"Loading model from {loss_path}\")\n",
    "    checkpoint = torch.load(loss_path, map_location=device)\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Best Loss: {checkpoint.get('best_loss', 'N/A')}\")\n",
    "    else:\n",
    "        model = checkpoint\n",
    "elif os.path.exists(old_path):\n",
    "    print(f\"Loading model from {old_path}\")\n",
    "    model = torch.load(old_path, map_location=device)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No model found! Train the model first.\")\n",
    "        \n",
    "model.to(device)\n",
    "model.eval()\n",
    "run_inference(test_dl, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
